{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Lab 2 Exercise 2a\n",
    "\n",
    "## Cost (Loss) Function\n",
    "\n",
    "You have learnt some cost functions commonly used in machine learning. In sum, in machine learning a cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X (input) and y (output)\n",
    "\n",
    "In the previous class, we have discussed:\n",
    "+ Mean Square Error\n",
    "+ Cross Entropy\n",
    "\n",
    "We are going to implement them using Python and Numpy.\n",
    "\n",
    "First, begin the code by importing numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the cell below to import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mean Square Error\n",
    "\n",
    "Recall the mean square error equation is\n",
    "\n",
    "\\begin{equation}\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^n (\\hat{Y_{i}} - Y_{i})^{2}\n",
    "\\end{equation}\n",
    "where $\\hat{Y_{i}}$ is the i-th prediction of the model and $\\ Y_{i}$ is the i-th ground truth (sample output)\n",
    "\n",
    "Sometimes, it is useful to further divide the result by 2, as in the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "\\end{equation}\n",
    "where $\\theta$ denotes the parameters\n",
    "\n",
    "Now try to implement the code below. You should use vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2a-1: Complete the following mean square error function in Python\n",
    "# You should have imported numpy. Make sure you have run the previous cell of import.\n",
    "\n",
    "def mean_square_loss(Y, Y_hat):\n",
    "    '''\n",
    "    Inputs - Y: vector / matrix of ground truths (outcomes)\n",
    "             Y_hat: vector / matrix of predicted values from a model\n",
    "    Return - Mean square error\n",
    "    '''\n",
    "    m = Y.shape[0]\n",
    "    ### Your codes here (2 - 3 lines) ###\n",
    "    ### Hints: Remember to divide the result by 2, as in the equation of J(theta) above\n",
    "    sq = \n",
    "    J = \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, we will generate for y and y_hat the column vector of length 5 with some random values and feed them into the mean_square_loss above. If your mean_square_loss function above is declared correctly, you should expect to see this output:\n",
    "\n",
    "**MSE loss =  0.0347007606832**\n",
    "\n",
    "Now run the following cell and compare the result. You do not have to change any codes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our output is the same\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate some random values of y and y^\n",
    "y = np.random.rand(5)\n",
    "y_hat = np.random.rand(5)\n",
    "\n",
    "# display y and y_hat data\n",
    "print(\"y     = \",y)\n",
    "print(\"y_hat = \",y_hat)\n",
    "\n",
    "# display the data type of y and y_hat\n",
    "print(\"Type of y     :\", type(y))\n",
    "print(\"Type of y_hat :\", type(y_hat))\n",
    "print(\"=\"*50)\n",
    "# run the mean_square_loss function you have implemented above\n",
    "print(\"MSE loss = \",mean_square_loss(y, y_hat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cross Entropy Loss\n",
    "\n",
    "Equation of cross entropy loss is\n",
    "\\begin{equation}\n",
    "J = -\\sum_{i=1}^{m} y_i\\log (h_\\theta(x_i)) + (1 - y_i)\\log(1 - h_\\theta(x_i))\n",
    "\\end{equation}\n",
    "\n",
    "where $h_\\theta(x_i)$ is the predicted output $(\\hat{Y_{i}})$ of the input sample $x_i$ after running through the model with some parameters $\\theta$.\n",
    "\n",
    "You are encouraged to look into the term \"Entropy\" and \"Cross Entropy\" for better understanding.\n",
    "\n",
    "Now try to implement the code below. Again you should use vectorized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2a-2: Complete the following cross entropy loss in Python\n",
    "# You should have imported numpy. Make sure you have run the previous cell of import.\n",
    "\n",
    "def cross_entropy_loss(Y, Y_hat):\n",
    "    '''\n",
    "    Inputs - Y: vector / matrix of ground truths (outcomes)\n",
    "             Y_hat: vector / matrix of predicted values from a model\n",
    "    Return - Cross Entropy Loss\n",
    "    '''\n",
    "    ### Your codes here (2 - 3 lines) ###\n",
    "    m = \n",
    "    J = \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation, we will generate for y and y_hat the column vector of length 5 with some random values and feed them into the mean_square_loss above. If your mean_square_loss function above is declared correctly, you should expect to see this output:\n",
    "\n",
    "**Cross Entropy loss =  0.925855227907**\n",
    "\n",
    "Now run the following cell and compare the result. You do not have to change any codes below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our output is the same\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate some random values of y and y^\n",
    "y = np.random.rand(5)\n",
    "y_hat = np.random.rand(5)\n",
    "\n",
    "# display y and y_hat data\n",
    "print(\"y     = \",y)\n",
    "print(\"y_hat = \",y_hat)\n",
    "\n",
    "# display the data type of y and y_hat\n",
    "print(\"Type of y     :\", type(y))\n",
    "print(\"Type of y_hat :\", type(y_hat))\n",
    "print(\"=\"*50)\n",
    "# run the mean_square_loss function you have implemented above\n",
    "print(\"Cross Entropy loss = \",cross_entropy_loss(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why vectorized?\n",
    "\n",
    "Using vectorized approach in calculation of cost function with the examples of column vectors above may not gain much speed than using traditional programming approach of for-loop. However, if the data size is huge, using for-loop to iterate through each sample is computational costly. We will compare the time taken on calculation using both approaches as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "# make sure our random values are the same\n",
    "np.random.seed(1)\n",
    "size = 1000000\n",
    "# generate some random values of y and y^\n",
    "y = np.random.rand(size)\n",
    "y_hat = np.random.rand(size)\n",
    "\n",
    "# we've implemented the cross entropy loss using for-loop as below\n",
    "def cross_entropy_for_loop(Y, Y_hat):\n",
    "    m = Y.shape[0]\n",
    "    acc = 0.0\n",
    "    for i in range(m):\n",
    "        tmp = Y[i] * math.log(Y_hat[i]) + (1 - Y[i]) * math.log(1 - Y_hat[i])\n",
    "        acc += tmp\n",
    "    return -acc / m\n",
    "\n",
    "# The following will run the cross entropy loss using both approaches and compare the running time required.\n",
    "start_time = time.time()\n",
    "J = cross_entropy_loss(y, y_hat)\n",
    "total_time = time.time() - start_time\n",
    "print(\"Cross Entropy loss (vectorized) = {:2.10}, total time taken = {:2.4}s\".format(J,total_time))\n",
    "\n",
    "start_time = time.time()\n",
    "J = cross_entropy_for_loop(y, y_hat)\n",
    "total_time = time.time() - start_time\n",
    "print(\"Cross Entropy loss (for-loop) = {:2.10}, total time taken = {:2.4}s\".format(J,total_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a input vector of size 1000000 above, even though the cross entropy losses calculated using both approaches are the same, using vectorized approach is indeed over 100 times faster than using traditional for-loop. Numpy plays a key role in doing all the matrix manipulation behind!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
